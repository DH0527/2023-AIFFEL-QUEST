{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. 신경망 시작하기: 분류와 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__분류와 회귀에서 사용하는 용어__  \n",
    "- **sample** 또는 **input**: 모델에 주입될 data point\n",
    "- **prediction** 또는 **output**: 모델로부터 나오는 값\n",
    "- **target**: 예측해야 하는 정답\n",
    "- **prediction error** 또는 **loss**: 모델의 예측과 정답 사이의 거리를 측정한 값\n",
    "- **class**: 분류 문제에서 선택가능한 레이블의 집합\n",
    "- **label**: 분류 문제에서 샘플에 할당된 클래스명\n",
    "- **ground-truth** 또는 **annotation**: 일반적으로 사람에 의해 수집되는 데이터셋의 타깃\n",
    "- **binary classification**: 각 입력 샘플을 2개의 서로 다른 클래스로 구분하는 작업\n",
    "- **multiclass classification**: 각 입력 샘플을 2개 이상의 서로 다른 클래스로 구분하는 작업\n",
    "- **multi-label classification**: 각 입력 샘플을 여러 개의 레이블에 할당하며 구분하는 작업\n",
    "- **scalar regression**: 회귀하는 스칼라 값을 찾는 작업\n",
    "- **vector regression**: 회귀하는 벡터 값을 찾는 작업\n",
    "- **mini-batch** 또는 **batch**: 모델에 의해 동시에 처리되는 샘플의 묶음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 영화 리뷰 분류: 이진 분류 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 IMDB 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB dataset load\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "positive = 1, negative = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# review decoding\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# 0 = padding, 1 = start, 2 = unknown으로 예약되어 있으므로 3을 빼준다.\n",
    "decoded_review = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망에는 동일한 길이의 입력들을 주어야 한다.  \n",
    "숫자 리스트는 다음과 같은 방법으로 텐서로 변환한다.\n",
    "- 리스트에 padding을 추가하여 (samples, max_length) 크기의 정수 텐서로 변환한다.  \n",
    "- 리스트를 multi-hot encoding하여 0과 1의 벡터로 변환한다.  \n",
    "  (samples, num_words) 크기의 정수 텐서로 변환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-hot encoding\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 신경망 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "from keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**활성화 함수가 필요한 이유**\n",
    "> output = dot(W, input) + b\n",
    "\n",
    "모델의 모든 층은 입력값에 대한 선형 변환이다.  \n",
    "가설 공간을 풍부하게 만들어 층을 깊게 만드는 장점을 살리기 위해서는 비선형성 또는 활성화 함수를 추가해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이진 분류 문제이므로 binary cross-entropy를 사용한다.  \n",
    "cross-entropy: 서로 다른 두 분포 간의 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compile\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 훈련 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing validation data set\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.5127 - accuracy: 0.7771 - val_loss: 0.4111 - val_accuracy: 0.8328\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3091 - accuracy: 0.8978 - val_loss: 0.3141 - val_accuracy: 0.8773\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.2291 - accuracy: 0.9231 - val_loss: 0.2901 - val_accuracy: 0.8843\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1853 - accuracy: 0.9377 - val_loss: 0.3219 - val_accuracy: 0.8690\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1547 - accuracy: 0.9477 - val_loss: 0.2804 - val_accuracy: 0.8875\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.1291 - accuracy: 0.9588 - val_loss: 0.2946 - val_accuracy: 0.8830\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.1099 - accuracy: 0.9667 - val_loss: 0.3048 - val_accuracy: 0.8832\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0950 - accuracy: 0.9720 - val_loss: 0.3202 - val_accuracy: 0.8815\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0823 - accuracy: 0.9746 - val_loss: 0.3444 - val_accuracy: 0.8800\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0668 - accuracy: 0.9829 - val_loss: 0.3637 - val_accuracy: 0.8799\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0602 - accuracy: 0.9837 - val_loss: 0.3815 - val_accuracy: 0.8770\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0489 - accuracy: 0.9886 - val_loss: 0.4056 - val_accuracy: 0.8743\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0436 - accuracy: 0.9886 - val_loss: 0.4329 - val_accuracy: 0.8746\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.0367 - accuracy: 0.9907 - val_loss: 0.4543 - val_accuracy: 0.8748\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0305 - accuracy: 0.9940 - val_loss: 0.4766 - val_accuracy: 0.8727\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0208 - accuracy: 0.9974 - val_loss: 0.5540 - val_accuracy: 0.8611\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0235 - accuracy: 0.9951 - val_loss: 0.7098 - val_accuracy: 0.8405\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0220 - accuracy: 0.9950 - val_loss: 0.5595 - val_accuracy: 0.8679\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0131 - accuracy: 0.9987 - val_loss: 0.7070 - val_accuracy: 0.8464\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.0152 - accuracy: 0.9976 - val_loss: 0.5928 - val_accuracy: 0.8708\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
